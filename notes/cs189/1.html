<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Note 1 - Classification</title>
  <meta name="description" content="TODO: LOGISTICAL STUFF">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://addcnin.blue/notes/cs189/1">
  
  
  <link rel="alternate" type="application/rss+xml" title="Addison Chan" href="https://addcnin.blue/feed.xml">

  
  <script>
    MathJax = {
loader: {load: ['[tex]/physics']},
  tex: {packages: {'[+]': ['physics']}},
      options: {
        renderActions: {
          /* add a new named action not to override the original 'find' action */
          find_script_mathtex: [10, function (doc) {
            for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            }
          }, '']
        }
      }
    };
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



  
  <meta property="og:title" content="Note 1 - Classification">
  <meta property="og:site_name" content="Addison Chan">
  <meta property="og:url" content="https://addcnin.blue/notes/cs189/1">
  <meta property="og:description" content="TODO: LOGISTICAL STUFF">
  
  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="addcninblue">
  <meta name="twitter:title" content="Note 1 - Classification">
  <meta name="twitter:description" content="TODO: LOGISTICAL STUFF">
  
    <meta name="twitter:creator" content="addcninblue">
  
  

  <link rel="dns-prefetch" href="https://fonts.gstatic.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css?family=Bitter:400,400i,700&display=swap" rel="stylesheet">

  
  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-89539680-2', 'auto');
    ga('send', 'pageview');

  </script>


</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/"><img src="/assets/me.jpg" id="me" alt=""> </img></a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/notes/">Notes</a>
      
        
        <a class="page-link" href="/teaching/">Teaching</a>
      
        
        <a class="page-link" href="/blog/">Blog</a>
      
        
        <a class="page-link" href="/projects/">Projects</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Note 1 - Classification</h1>
  </header>

  <div class="post-content">

    
    <div class="table-of-contents">
    <hr>
    <h2 class="table-of-contents"> Table of Contents </h2>
      <ul>
  <li><a href="#todo-logistical-stuff">TODO: LOGISTICAL STUFF</a></li>
  <li><a href="#some-motivation-for-classification">Some Motivation For Classification</a>
    <ul>
      <li><a href="#example-credit-card-default">Example: Credit Card Default</a></li>
    </ul>
  </li>
  <li><a href="#k-nearest-neighbors">K-Nearest-Neighbors</a></li>
  <li><a href="#classifying-digits">Classifying Digits</a></li>
  <li><a href="#validation">Validation</a></li>
  <li><a href="#kaggle-logistics">Kaggle Logistics</a></li>
</ul>

    </div>
    

    

    <hr>

    <h2 id="todo-logistical-stuff">TODO: LOGISTICAL STUFF</h2>

<h2 id="some-motivation-for-classification">Some Motivation For Classification</h2>
<ul>
  <li>Can nearly train a driving AI with classification! (??)</li>
  <li>Steps for classification: ## TODO: insert picture
    <ol>
      <li>Collect training data: reliable debtors and defaulted debtors</li>
      <li>Evaluate new applicants (prediction)</li>
    </ol>
  </li>
</ul>

<hr class="hr-subsection" />

<h3 id="example-credit-card-default">Example: Credit Card Default</h3>
<ul>
  <li>Suppose we have the above data. We can create a <strong>linear classifier</strong> by drawing this line as follows:</li>
  <li>Alternatively, we can have a nearest-neighbors approach, where we <em>predict a result by looking at its nearest neighbor in Euclidean Space</em>.</li>
</ul>

<hr class="hr-subsection" />

<ul>
  <li>Decision boundaries: the decision boundary is the line that separates one group from another. In the example below, it is the line in green.
    <ul>
      <li>Advantages of a nearest neighbor classifier: Every point is well-classified; <em>they all fall on the right side of the boundary</em>.</li>
      <li>Advantages of a linear (ie. not as well-fitting decision boundary): It can have more predictive power. Even though the well-fitting decision boundary is good at predicting training data, it might be bad at predicting future points.</li>
      <li>Summary: We don’t know if the process that creates points and classes will be exactly what the well-fitting decision boundary gives. This phenomenon is called <strong>overfitting</strong>.</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="k-nearest-neighbors">K-Nearest-Neighbors</h2>
<ul>
  <li>Take a number <code class="highlighter-rouge">k</code>, and look at the <code class="highlighter-rouge">k</code> nearest Euclidean neighbors as a prediction. In the case of lecture, a 15-nearest-neighbor classification improves upon on the pure 1-nearest-neighbor approach.</li>
  <li>A note: We <em>always</em> want <code class="highlighter-rouge">k</code> to be odd, so we can have tiebreakers.</li>
  <li><code class="highlighter-rouge">k</code> is an example of a <strong>hyperparameter</strong></li>
</ul>

<hr />

<h2 id="classifying-digits">Classifying Digits</h2>
<ul>
  <li>Suppose we have an input vector of digits. Images are points in 16-dimensional space; ie. <script type="math/tex">\mathbb{R}^{16}</script> . A linear decision boundary is a <em>hyperplane</em>.</li>
</ul>

<h2 id="validation">Validation</h2>
<ul>
  <li>Steps:
    <ol>
      <li>Train a classifier: it learns to distinguish 7 from not 7.</li>
      <li>Test the classifier on <em>new images</em>.</li>
    </ol>
  </li>
  <li>Kinds of error:
    <ul>
      <li><strong>Training set error</strong>: Fraction of <em>training set images</em> not classified correctly by the model.</li>
      <li><strong>Test set error</strong>: Fraction of <em>new images</em> not classified correctly by the model.</li>
    </ul>
  </li>
  <li>A few definitions:
    <ul>
      <li><strong>Outliers</strong>: points whose labels are atypical – mislabeled points, bad data, or just irregular data</li>
      <li><strong>Overfitting</strong>: when the test error deteriorates because the classifier <em>becomes too sensitive to outliers or other spurious patterns</em>.</li>
      <li><strong>Generalizing</strong>:</li>
      <li><strong>Hyperparameter</strong>: Most ML algorithms have a few tunable parameters that control overfitting or underfitting, eg. <code class="highlighter-rouge">k</code> in k-nearest-neighbors.
        <ul>
          <li>In k-nearest-neighbors, we can see underfitting when <code class="highlighter-rouge">k</code> is small, and overfitting when <code class="highlighter-rouge">k</code> is too large. There appears to be a best <code class="highlighter-rouge">k</code> in the middle. There is a process of <em>validation</em> that finds the best parameter in the middle.</li>
          <li>##TODO: image on training error</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>And onto <strong>validation</strong>: A process to select hyperparameters
    <ul>
      <li>Hold back a subset of the labeled data, called the <strong>validation set</strong>.</li>
      <li>Train the classifier multiple times with different hyperparameter settings. Alternatively, train multiple different classifiers with different hyperparameters.</li>
      <li>Choose the settings that work best on the validation set.</li>
    </ul>
  </li>
  <li>Now we have 3 sets:
    <ul>
      <li><strong>Training set</strong>: used to learn model weights</li>
      <li><strong>Validation set</strong>: used to tune hyperparameters, and to choose a model (ie. linear vs knn)</li>
      <li><strong>Test set</strong>: used as a <em>final</em> evaluation of a model. Keep it in a vault. Run ONCE, at the very end.</li>
    </ul>
  </li>
</ul>

<h2 id="kaggle-logistics">Kaggle Logistics</h2>
<ul>
  <li>Runs ML competitions, including our homeworks.</li>
  <li>We use 2 data sets:
    <ul>
      <li>“public” set: labels available during competition
        <ul>
          <li>Goes to the <strong>training</strong> and <strong>validation</strong> set</li>
        </ul>
      </li>
      <li>“private” set: revealed only after due date
        <ul>
          <li>Goes to the <strong>test</strong> set</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>


  </div>

  

</article>

      </div>
    </main>

    
    
    

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

@addcninblue

    </p>

  </div>

</footer>


  </body>

</html>
