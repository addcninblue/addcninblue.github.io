---
layout: note
title: "Note 1 - Classification"
toc: true
note: 1
mathjax: true
---
## TODO: LOGISTICAL STUFF

## Some Motivation For Classification
- Can nearly train a driving AI with classification! (??)
- Steps for classification: ## TODO: insert picture
    1. Collect training data: reliable debtors and defaulted debtors
    1. Evaluate new applicants (prediction)

---
{: .hr-subsection}

### Example: Credit Card Default
- Suppose we have the above data. We can create a **linear classifier** by drawing this line as follows:
- Alternatively, we can have a nearest-neighbors approach, where we *predict a result by looking at its nearest neighbor in Euclidean Space*.

---
{: .hr-subsection}

- Decision boundaries: the decision boundary is the line that separates one group from another. In the example below, it is the line in green.
    - Advantages of a nearest neighbor classifier: Every point is well-classified; *they all fall on the right side of the boundary*.
    - Advantages of a linear (ie. not as well-fitting decision boundary): It can have more predictive power. Even though the well-fitting decision boundary is good at predicting training data, it might be bad at predicting future points.
    - Summary: We don't know if the process that creates points and classes will be exactly what the well-fitting decision boundary gives. This phenomenon is called **overfitting**.

---

## K-Nearest-Neighbors
- Take a number `k`, and look at the `k` nearest Euclidean neighbors as a prediction. In the case of lecture, a 15-nearest-neighbor classification improves upon on the pure 1-nearest-neighbor approach.
- A note: We _always_ want `k` to be odd, so we can have tiebreakers.
- `k` is an example of a **hyperparameter**

---

## Classifying Digits
- Suppose we have an input vector of digits. Images are points in 16-dimensional space; ie. $$\mathbb{R}^{16}$$ . A linear decision boundary is a *hyperplane*.

## Validation
- Steps:
    1. Train a classifier: it learns to distinguish 7 from not 7.
    1. Test the classifier on *new images*.
- Kinds of error:
    - **Training set error**: Fraction of *training set images* not classified correctly by the model.
    - **Test set error**: Fraction of *new images* not classified correctly by the model.
- A few definitions:
    - **Outliers**: points whose labels are atypical -- mislabeled points, bad data, or just irregular data
    - **Overfitting**: when the test error deteriorates because the classifier *becomes too sensitive to outliers or other spurious patterns*.
    - **Generalizing**: 
    - **Hyperparameter**: Most ML algorithms have a few tunable parameters that control overfitting or underfitting, eg. `k` in k-nearest-neighbors.
        - In k-nearest-neighbors, we can see underfitting when `k` is small, and overfitting when `k` is too large. There appears to be a best `k` in the middle. There is a process of *validation* that finds the best parameter in the middle.
        - ##TODO: image on training error
- And onto **validation**: A process to select hyperparameters
    - Hold back a subset of the labeled data, called the **validation set**.
    - Train the classifier multiple times with different hyperparameter settings. Alternatively, train multiple different classifiers with different hyperparameters.
    - Choose the settings that work best on the validation set.
- Now we have 3 sets:
    - **Training set**: used to learn model weights
    - **Validation set**: used to tune hyperparameters, and to choose a model (ie. linear vs knn)
    - **Test set**: used as a *final* evaluation of a model. Keep it in a vault. Run ONCE, at the very end.

## Kaggle Logistics
- Runs ML competitions, including our homeworks.
- We use 2 data sets:
    - "public" set: labels available during competition
        - Goes to the **training** and **validation** set
    - "private" set: revealed only after due date
        - Goes to the **test** set

