<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Incremental Gain, Monumental Pain</title>
  <meta name="description" content="A common paradigm of software engineering is making progress continuously rather than discretely. By this, I mean that given an ideal, most would prefer to make small edits that get them closer to the ideal rather than a large change all at once. This means that the workflow for change looks like the following: There is a realization that something is locally wrong. The root of the problem is identified. A change is implemented that fixes the problem.">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://addcnin.blue/2023/01/22/incremental-progress/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Addison Chan" href="https://addcnin.blue/feed.xml">

  


  
  <meta property="og:title" content="Incremental Gain, Monumental Pain">
  <meta property="og:site_name" content="Addison Chan">
  <meta property="og:url" content="https://addcnin.blue/2023/01/22/incremental-progress/">
  <meta property="og:description" content="A common paradigm of software engineering is making progress continuously rather than discretely. By this, I mean that given an ideal, most would prefer to make small edits that get them closer to the ideal rather than a large change all at once. This means that the workflow for change looks like the following: There is a realization that something is locally wrong. The root of the problem is identified. A change is implemented that fixes the problem.">
  
  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="addcninblue">
  <meta name="twitter:title" content="Incremental Gain, Monumental Pain">
  <meta name="twitter:description" content="A common paradigm of software engineering is making progress continuously rather than discretely. By this, I mean that given an ideal, most would prefer to make small edits that get them closer to ...">
  
    <meta name="twitter:creator" content="addcninblue">
  
  

  <link rel="dns-prefetch" href="https://fonts.gstatic.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Bitter:ital,wght@0,400;0,700;1,400&amp;display=swap" rel="stylesheet">

  
  
  
    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-89539680-2', 'auto');
      ga('send', 'pageview');
    </script>
  


</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/"><img src="/assets/me.jpg" id="me" alt=""> </img></a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/interesting/">&nbsp</a>
      
        
        <a class="page-link" href="/notes/">Notes</a>
      
        
        <a class="page-link" href="/blog/">Blog</a>
      
        
        <a class="page-link" href="https://github.com/addcninblue/">GitHub</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
      <h1 class="post-title" itemprop="name headline">Incremental Gain, Monumental Pain</h1>
    
    <p class="post-meta"><time datetime="2023-01-22T00:00:00-08:00" itemprop="datePublished">Jan 22, 2023</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>A common paradigm of software engineering is making progress continuously rather
than discretely. By this, I mean that given an ideal, most would prefer to make
small edits that get them closer to the ideal rather than a large change all at
once. This means that the workflow for change looks like the following:</p>
<ol>
  <li>There is a realization that something is locally wrong.</li>
  <li>The root of the problem is identified.</li>
  <li>A change is implemented that fixes the problem.</li>
</ol>

<p>This belief has its merits. After all, for most systems, it’s often unclear what
the eventual state of the system should look like. It’s easy to cut down on
known ugly parts of the system, since knowing what it <em>shouldn’t</em> look like
amounts somewhat to knowing what it <em>should</em> look like.</p>

<p>But in reflection, we often find pieces of software in utter disarray.
Reasonable questions such as “How did we get here” end with answers like “We
made the best call at every turn, but somehow we ended in a suboptimal state.”
To be clear, it’s not always as if corners were cut to ship a feature.
Sometimes, it’s a partial refactor; other times, it’s a feature slapped on top
of infrastructure not designed to support it. At the time the shots were called,
these were always the right business decisions, but they somehow converged to a
less-than-suboptimal state.</p>

<p>So what happened?</p>

<h2 id="a-machine-learning-analogy">A Machine Learning Analogy</h2>

<p>To take an analogy from machine learning, if we think of a program as something
solving a problem, we can also think of it as minimizing some objective
function. For example, an objective function might be minimizing the manual
labor required.</p>

<p>In this analogy, making small changes at a time to a system amounts to doing
gradient descent on a somewhat unknown objective function. Concretely, the three
steps above would look like the following:</p>
<ol>
  <li>There is a realization that the program is not producing output in a way that
minimizes the objective function. In following the example above, it might be
that manual intervention is higher than it needs to be.</li>
  <li>The immediate cause is identified. In the example above, that might be
something as simple as realizing that previous automation has since been
broken.</li>
  <li>A software engineer implements a fix. In the example above, that might be
fixing the previous automation.</li>
</ol>

<p>Before we continue, let’s address some things. First, why was this described as
a “somewhat” unknown objective function? This is largely a criticism of step 2.
When problems are discovered, it’s often the case that everyone is acutely aware
of the immediate problem, but that the broader or deeper problem is less
obvious. In this case, we can say that the objective function is locally
obvious, but globally unknown.</p>

<p>We know how this ends with machine learning. If a learning algorithm is only
allowed to explore a local space, it will always converge only within the
confines of that space. In our example here, it means that we will only solve
problems that we identify. In such a case, the foregone conclusion would be to
consider a wider range of problems than the one at hand. That’s perhaps an
obvious statement, but a deeper example is more interesting.</p>

<p>There is a more insidious variation that I think is more prevalent. Consider the
case where the global objective function is identified in step #2 rather than
only the local objective function. Furthermore, assume that there exists a
global minimum that is not close to the existing state; that is, there exists a
local minimum that is closer than the global minimum to the existing state, and
that moving from the existing state to the global one requires performing worse
on the objective function before performing better.</p>

<p>In such a case, we are faced with two possibilities: go for the local optimum,
in which case a smaller amount of work is required; or go for the global
optimum, in which case much planning is required and much risk is taken on, but
perhaps it gets us closer to the global minimum.</p>

<p>Most would choose the first option: go for the local optimum. We call this
“incremental improvement.” But most also never make the connection that they can
never “incrementally improve” towards the global optimum under such
circumstances; they will always be in a <em>better</em> state than they were before,
but there will exist pain points which cannot be fixed.</p>

<h2 id="so-what-to-do">So What To Do</h2>

<p>It’s a curious balance of optima and work that we have to balance. We have on
one hand the comfort of our own optimum “well,” where we know which problems we
have yet to solve and those which are unsolvable. We have on the other hand the
ideal “global minimum well” where there are undoubtedly some dragons lurking
accompanied with the hope of a brighter future. I don’t have a silver bullet as
to which to choose, but I do have some thoughts, followed by recommendations:</p>

<p>Thoughts:</p>
<ul>
  <li>I think if software ever hits the point of “How did we get here?”, some
  horrible mistake was made. It means that someone either forgot the
  difference between a local and global optimum, or that they thought the
  optimum they were optimizing for was the global one.</li>
  <li>This means that it’s quite important to understand the landscape before making
  changes. As they say, a month in the laboratory can often save an hour in
  the library!</li>
  <li>Expanding on the above, sometimes the correct choice <em>is</em> to optimize locally
  even if the eventual plan is to make a discrete jump to better optimize
  globally – it just so happens that accumulating more pain points informs
  you about the global objective function.</li>
</ul>

  </div>

  

</article>

      </div>
    </main>

    
    
    

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

@addcninblue

    </p>

  </div>

</footer>


  </body>

</html>
